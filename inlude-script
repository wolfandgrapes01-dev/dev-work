import os
import re
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm
import io

# --------------------------
# 設定
# --------------------------
root_dir = r"C:\path\to\your\root"  # ルートディレクトリ（実パスに差し替えてください）
project_name = "ProjectA"            # 解析対象プロジェクト
extensions = ('.cpp', '.c', '.h', '.hpp')
include_pattern = re.compile(r'#include\s+[<"](.+?)[>"]')

# --------------------------
# 1️⃣ ファイルインデックス作成（root 下の全ファイルを登録）
# --------------------------
file_index = defaultdict(list)
for dirpath, dirnames, filenames in os.walk(root_dir):
    dirnames[:] = [d for d in dirnames if d.lower() != ".svn"]
    for filename in filenames:
        if filename.endswith(extensions):
            file_index[filename].append(os.path.abspath(os.path.join(dirpath, filename)))

# --------------------------
# 2️⃣ ProjectA/src 下の解析対象ファイルを収集（トップレベル）
# --------------------------
project_a_src = os.path.join(root_dir, project_name, "src")
top_files = []
for dirpath, dirnames, filenames in os.walk(project_a_src):
    dirnames[:] = [d for d in dirnames if d.lower() != ".svn"]
    for filename in filenames:
        if filename.endswith(extensions):
            top_files.append(os.path.abspath(os.path.join(dirpath, filename)))

# --------------------------
# 3️⃣ root 下の全ファイル集合（グローバル重複除去用）
# --------------------------
all_root_files = set()
for dirpath, dirnames, filenames in os.walk(root_dir):
    dirnames[:] = [d for d in dirnames if d.lower() != ".svn"]
    for filename in filenames:
        if filename.endswith(extensions):
            all_root_files.add(os.path.abspath(os.path.join(dirpath, filename)))

global_seen_in_root = set()  # root 下の依存ファイルをグローバルに一度だけ採用するためのセット

# --------------------------
# 4️⃣ ファイル解析関数（1ファイル分の直接 include を抽出）
#    - .h に対して同名 .cpp を補完して includes に追加する（まだ未解析なら次ラウンドで処理）
# --------------------------
def parse_file_once(full_path):
    includes_list = []
    try:
        with open(full_path, 'r', encoding='utf-8', errors='ignore') as f:
            in_block_comment = False
            for line in f:
                stripped = line.strip()
                # ブロックコメント内かどうかの処理
                if in_block_comment:
                    if '*/' in stripped:
                        in_block_comment = False
                        stripped = stripped.split('*/', 1)[1]
                    else:
                        continue
                if '/*' in stripped:
                    if '*/' in stripped:
                        stripped = re.sub(r'/\*.*?\*/', '', stripped)
                    else:
                        in_block_comment = True
                        stripped = stripped.split('/*', 1)[0]
                # 行コメント除去
                stripped = re.sub(r'//.*', '', stripped).strip()
                if not stripped:
                    continue

                match = include_pattern.search(stripped)
                if not match:
                    continue
                include_file = match.group(1)

                # システムヘッダや絶対パスの include は無視
                if include_file.startswith('<') or include_file.startswith('/'):
                    continue

                # 同ディレクトリ優先で絶対パス候補を作る
                candidate = os.path.join(os.path.dirname(full_path), include_file)
                if os.path.exists(candidate):
                    include_path = os.path.abspath(candidate)
                else:
                    basename = os.path.basename(include_file)
                    if basename in file_index and file_index[basename]:
                        include_path = file_index[basename][0]
                    else:
                        # プロジェクト内で見つからない場合は無視（外部ライブラリ等）
                        continue

                # root 内ファイルについてはグローバル一度だけ採用するルール
                if include_path in all_root_files:
                    if include_path not in global_seen_in_root:
                        global_seen_in_root.add(include_path)
                        includes_list.append(include_path)
                    else:
                        # 既にグローバルで見たファイルは重複としてスキップ
                        continue
                else:
                    # root 外のファイルはそのまま追加（重複排除の対象外）
                    includes_list.append(include_path)

        # .h/.hpp に対して同名の .cpp を補完する（あれば追加）
        if full_path.endswith(('.h', '.hpp')):
            base = os.path.splitext(os.path.basename(full_path))[0]
            cpp_name = base + ".cpp"
            if cpp_name in file_index and file_index[cpp_name]:
                cpp_path = file_index[cpp_name][0]
                # cpp_path が root 内でまだグローバル採用されていなければ採用して includes に追加
                if cpp_path in all_root_files:
                    if cpp_path not in global_seen_in_root:
                        global_seen_in_root.add(cpp_path)
                        includes_list.append(cpp_path)
                    else:
                        # 既にグローバルで採用済み -> スキップ
                        pass
                else:
                    # root 外の cpp（稀）も追加
                    includes_list.append(cpp_path)

    except Exception as e:
        print(f"⚠️ 読み取り失敗: {full_path} - {e}")

    return full_path, includes_list

# --------------------------
# 5️⃣ 再帰的に並列解析するループ
#    - pending: 今回解析すべきファイル集合（初回は top_files）
#    - parsed: 既に解析済みのファイル
#    - next_pending に新しく発見した未解析ファイルを入れて次ラウンドへ
# --------------------------
dependency_map = {}
parsed = set()
pending = set(top_files)

print(f"🔍 ProjectA/src のトップレベル {len(top_files)} 個から再帰解析を開始します...")

with ThreadPoolExecutor(max_workers=os.cpu_count() or 4) as executor:
    round_idx = 0
    while pending:
        round_idx += 1
        pending_list = list(pending)
        pending = set()
        desc = f"解析ラウンド {round_idx} ({len(pending_list)} 件)"
        for full_path, includes in tqdm(executor.map(parse_file_once, pending_list), total=len(pending_list), desc=desc):
            # dependency_map に結果を入れる（上書きしても問題ない）
            dependency_map[full_path] = includes
            parsed.add(full_path)

            # includes に含まれるファイルがプロジェクト内でまだ解析されていなければ次ラウンドに追加
            for dep in includes:
                if isinstance(dep, str) and os.path.isabs(dep) and os.path.exists(dep):
                    if dep not in parsed:
                        pending.add(dep)
        # 次ラウンドへ（pending が空になるまで続く）

# --------------------------
# 6️⃣ ツリー生成（依存関係テーブルは完全にそろっているため、再帰で最底層まで展開できる）
# --------------------------
def build_tree_lines(file_path, prefix='', visited=None):
    if visited is None:
        visited = set()
    if file_path in visited:
        return [prefix + "(循環依存)"]
    visited.add(file_path)

    lines = []
    deps = dependency_map.get(file_path, [])
    count = len(deps)
    for i, dep in enumerate(deps):
        branch = "└─ " if i == count - 1 else "├─ "
        sub_prefix = "   " if i == count - 1 else "│  "
        lines.append(prefix + branch + dep)
        if dep in dependency_map:
            lines.extend(build_tree_lines(dep, prefix + sub_prefix, visited))
    visited.remove(file_path)
    return lines

# --------------------------
# 7️⃣ TXT 書き出し（生成しながら書き込み、プログレス表示）
# --------------------------
output_file = os.path.join(root_dir, f"{project_name}_dependencies_tree.txt")

def estimate_total_lines(dep_map, roots):
    total = 0
    def count_lines(file_path, visited=None):
        nonlocal total
        if visited is None:
            visited = set()
        if file_path in visited:
            total += 1
            return
        visited.add(file_path)
        total += 1
        for dep in dep_map.get(file_path, []):
            count_lines(dep, visited)
        visited.remove(file_path)
    for f in roots:
        if f in dep_map:
            count_lines(f)
        else:
            total += 1  # 依存無しのトップファイルも1行とみなす
    return total

total_lines = estimate_total_lines(dependency_map, top_files)

with io.open(output_file, 'w', encoding='utf-8', buffering=1024*1024) as f:
    pbar = tqdm(total=total_lines, desc="TXT 書き込み進行中")
    for top_file in top_files:
        # トップファイルは top_files の順で出力（解析結果がなくても出力）
        f.write(top_file + "\n")
        pbar.update(1)
        if top_file in dependency_map:
            tree_lines = build_tree_lines(top_file)
            for line in tree_lines:
                f.write(line + "\n")
                pbar.update(1)
        f.write("\n")
        pbar.update(1)
    pbar.close()

print(f"\n✅ {project_name} の依存ツリーを完全に出力しました：{output_file}")